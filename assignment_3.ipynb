{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Input_data/cpa_preprocessed.csv')\n",
    "\n",
    "# Drop target variable\n",
    "features = data.drop(columns=['NumStorePurchases'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set default font size for all plots\n",
    "# plt.rcParams.update({\n",
    "#     'font.size': 18,\n",
    "#     'axes.titlesize': 18,\n",
    "#     'axes.labelsize': 18,\n",
    "#     'xtick.labelsize': 16,\n",
    "#     'ytick.labelsize': 16,\n",
    "#     'legend.fontsize': 18,\n",
    "#     'figure.titlesize': 22\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test = train_test_split(normalized_features, test_size=0.2, random_state=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine optimal number of clusters using Elbow Method and Silhouette Score\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 15)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=30, n_init=15)\n",
    "    kmeans.fit(X_train)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_train, kmeans.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 4))\n",
    "# plt.subplot(1, 2, 1)\n",
    "fig1, axs = plt.subplots(1, 2)\n",
    "fig1.suptitle('KMeans Results for CPA Dataset (No Dimensionality Reduction)')\n",
    "\n",
    "axs[0].plot(k_values, inertia, marker='o')\n",
    "axs[0].set_xlabel('Number of Clusters')\n",
    "axs[0].set_ylabel('Inertia')\n",
    "axs[0].set_title('Elbow Method')\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "# plt.subplot(1, 2, 2)\n",
    "axs[1].plot(k_values, silhouette_scores, marker='o', color='red')\n",
    "axs[1].set_xlabel('Number of Clusters')\n",
    "axs[1].set_ylabel('Silhouette Score')\n",
    "axs[1].set_title('Silhouette Score')\n",
    "\n",
    "\n",
    "# fig1.tight_layout()\n",
    "# fig1.subplots_adjust(top=0.88)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Optimal number of clusters (choosing the best from both methods)\n",
    "optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f'Optimal number of clusters: {optimal_k}')\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=30, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_train)\n",
    "\n",
    "# Apply Expectation Maximization (Gaussian Mixture Model)\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=30)\n",
    "gmm_labels = gmm.fit_predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate clustering performance\n",
    "kmeans_silhouette = silhouette_score(X_train, kmeans_labels)\n",
    "kmeans_db = davies_bouldin_score(X_train, kmeans_labels)\n",
    "\n",
    "gmm_silhouette = silhouette_score(X_train, gmm_labels)\n",
    "gmm_db = davies_bouldin_score(X_train, gmm_labels)\n",
    "gmm_log_likelihood = gmm.score(X_train)\n",
    "\n",
    "print(f'KMeans - Silhouette Score: {kmeans_silhouette}, Davies-Bouldin Index: {kmeans_db}')\n",
    "print(f'GMM - Silhouette Score: {gmm_silhouette}, Davies-Bouldin Index: {gmm_db}, Log-Likelihood: {gmm_log_likelihood}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Visualize clusters using PCA (2D)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=kmeans_labels, palette='viridis')\n",
    "plt.title('KMeans Clusters (PCA Reduced)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=gmm_labels, palette='coolwarm')\n",
    "plt.title('GMM Clusters (PCA Reduced)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Visualize clusters using PCA (3D)\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_train)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=kmeans_labels, cmap='viridis')\n",
    "ax.set_title('KMeans Clusters (PCA Reduced)')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=gmm_labels, cmap='coolwarm')\n",
    "ax.set_title('GMM Clusters (PCA Reduced)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding ideal number of clusters using GMM for CPA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Determine optimal number of clusters using GMM (BIC/AIC)\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "\n",
    "# cpa_gmm_inertia = []\n",
    "# cpa_gmm_silhouette_scores =[]\n",
    "\n",
    "k_values = range(2, 15)\n",
    "log_likelihoods = []\n",
    "\n",
    "for k in k_values:\n",
    "    gmm = GaussianMixture(n_components=k, random_state=30)\n",
    "    gmm.fit(X_train)\n",
    "    bic_scores.append(gmm.bic(X_train))\n",
    "    aic_scores.append(gmm.aic(X_train))\n",
    "    log_likelihoods.append(gmm.score(X_train))\n",
    "    # cpa_gmm_inertia.append(gmm.inertia_)\n",
    "    # cpa_gmm_silhouette_scores.append(silhouette_score(X_train, gmm.labels_))\n",
    "\n",
    "\n",
    "## there is no straight forward way to calculate intertia or silhouette score for GMM\n",
    "## so we will use BIC and AIC scores to determine the optimal number of clusters\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create DataFrame for BIC, AIC, and Log-Likelihood\n",
    "scores_df = pd.DataFrame({'Clusters': k_values, 'BIC': bic_scores, 'AIC': aic_scores, 'Log-Likelihood': log_likelihoods})\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Plot BIC and AIC Scores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, bic_scores, marker='o', color='blue')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.title('BIC Score for GMM')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, aic_scores, marker='o', color='red')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('AIC Score')\n",
    "plt.title('AIC Score for GMM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Plot Log-Likelihood\n",
    "plt.figure(figsize=(6, 4)) \n",
    "plt.plot(k_values, log_likelihoods, marker='o', color='green')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('Log-Likelihood for GMM')\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Optimal number of clusters (choosing the best from BIC)\n",
    "optimal_k = k_values[bic_scores.index(min(bic_scores))]\n",
    "print(f'Optimal number of clusters (GMM - BIC): {optimal_k}')\n",
    "\n",
    "# Apply Expectation Maximization (Gaussian Mixture Model)\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=30)\n",
    "gmm_labels = gmm.fit_predict(X_train)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "gmm_silhouette = silhouette_score(X_train, gmm_labels)\n",
    "gmm_db = davies_bouldin_score(X_train, gmm_labels)\n",
    "gmm_log_likelihood = gmm.score(X_train)\n",
    "\n",
    "print(f'GMM - Silhouette Score: {gmm_silhouette}, Davies-Bouldin Index: {gmm_db}, Log-Likelihood: {gmm_log_likelihood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Visualize clusters using PCA (2D)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=gmm_labels, palette='tab20')\n",
    "plt.title('GMM Clusters (PCA Reduced)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Visualize clusters using PCA (3D)\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_train)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=gmm_labels, cmap='tab20')\n",
    "ax.set_title('GMM Clusters (PCA Reduced)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spotify Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "spotify_data = pd.read_csv('Input_data/spotify_processed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spotify_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Replace NaN values in the 'in_shazam_charts' column with its median\n",
    "spotify_data['in_shazam_charts'] = spotify_data['in_shazam_charts'].fillna(spotify_data['in_shazam_charts'].median())\n",
    "\n",
    "# Verify that there are no more NaN values in the 'in_shazam_charts' column\n",
    "nan_counts_after = spotify_data['in_shazam_charts'].isnull().sum()\n",
    "print(f\"NaN values in 'in_shazam_charts' after replacement: {nan_counts_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Identify NaN values in 'key' and 'mode' columns\n",
    "print(\"NaN values before imputation:\")\n",
    "print(spotify_data[['key', 'mode']].isnull().sum())\n",
    "\n",
    "# Fill NaN values with most frequent value for each column\n",
    "spotify_data['key'] = spotify_data['key'].fillna(spotify_data['key'].mode()[0])\n",
    "spotify_data['mode'] = spotify_data['mode'].fillna(spotify_data['mode'].mode()[0])\n",
    "\n",
    "# Verify that there are no more NaN values\n",
    "print(\"\\nNaN values after imputation:\")\n",
    "print(spotify_data[['key', 'mode']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_features = spotify_data.drop(columns=['popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_features_encoded = pd.get_dummies(spotify_features, columns=['key', 'mode'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spotify_features_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spotify_features_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data\n",
    "spotitfy_scalar = StandardScaler()\n",
    "normalized_features = spotitfy_scalar.fit_transform(spotify_features_encoded)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_spotify, X_test_spotify = train_test_split(normalized_features, test_size=0.2, random_state=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine optimal number of clusters using Elbow Method and Silhouette Score\n",
    "inertia_spotify = []\n",
    "silhouette_scores_spotify = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = range(2, 14)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans_spotify = KMeans(n_clusters=k, random_state=30, n_init=10)\n",
    "    kmeans_spotify.fit(X_train_spotify)\n",
    "    inertia_spotify.append(kmeans_spotify.inertia_)\n",
    "    silhouette_scores_spotify.append(silhouette_score(X_train_spotify, kmeans_spotify.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, inertia_spotify, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouette_scores_spotify, marker='o', color='red')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Optimal number of clusters (choosing the best from both methods)\n",
    "optimal_k = k_values[silhouette_scores_spotify.index(max(silhouette_scores_spotify))]\n",
    "print(f'Optimal number of clusters: {optimal_k}')\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=30, n_init=10)\n",
    "spotify_kmeans_labels = kmeans.fit_predict(X_train_spotify)\n",
    "\n",
    "# Apply Expectation Maximization (Gaussian Mixture Model)\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=30)\n",
    "spotify_gmm_labels = gmm.fit_predict(X_train_spotify)\n",
    "\n",
    "# Evaluate clustering performance\n",
    "kmeans_silhouette = silhouette_score(X_train_spotify, spotify_kmeans_labels)\n",
    "kmeans_db = davies_bouldin_score(X_train_spotify, spotify_kmeans_labels)\n",
    "\n",
    "gmm_silhouette = silhouette_score(X_train_spotify, spotify_gmm_labels)\n",
    "gmm_db = davies_bouldin_score(X_train_spotify, spotify_gmm_labels)\n",
    "gmm_log_likelihood = gmm.score(X_train_spotify)\n",
    "\n",
    "print(f'KMeans - Silhouette Score: {kmeans_silhouette}, Davies-Bouldin Index: {kmeans_db}')\n",
    "print(f'GMM - Silhouette Score: {gmm_silhouette}, Davies-Bouldin Index: {gmm_db}, Log-Likelihood: {gmm_log_likelihood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Visualize clusters using PCA (2D)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_spotify)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=spotify_kmeans_labels, palette='viridis')\n",
    "plt.title('KMeans Clusters (PCA Reduced)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=spotify_gmm_labels, palette='coolwarm')\n",
    "plt.title('GMM Clusters (PCA Reduced)')\n",
    "plt.show()\n",
    "\n",
    "# Visualize clusters using PCA (3D)\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_train_spotify)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=spotify_kmeans_labels, cmap='viridis')\n",
    "ax.set_title('KMeans Clusters (PCA Reduced)')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=spotify_gmm_labels, cmap='coolwarm')\n",
    "ax.set_title('GMM Clusters (PCA Reduced)')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
